# src/data_managers/instrument_data_manager.py

import sys
import os

# Path dÃ¼zeltmesi - src/data_managers/ dizininden proje kÃ¶kÃ¼ne eriÅŸim
current_dir = os.path.dirname(os.path.abspath(__file__))  # data_managers/
parent_dir = os.path.dirname(current_dir)  # src/
project_root = os.path.dirname(parent_dir)  # proje kÃ¶kÃ¼
sys.path.insert(0, project_root)

print(f"ğŸ“‚ Proje kÃ¶k dizini: {project_root}")
print(f"ğŸ“‚ Mevcut dizin: {current_dir}")

import librosa
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
from sklearn.model_selection import train_test_split, StratifiedKFold
import json
import shutil
from tqdm import tqdm
import pickle
from collections import Counter
import tensorflow as tf
import tensorflow_hub as hub


class DataManager:
    """
    Good Sounds veri seti iÃ§in geliÅŸtirilmiÅŸ enstrÃ¼man tanÄ±ma veri yÃ¶netimi sÄ±nÄ±fÄ±
    YAMNet embeddings ve traditional features desteÄŸi
    """

    def __init__(self, base_dir: str = "data"):
        """
        DataManager baÅŸlatÄ±cÄ±sÄ±

        Args:
            base_dir: Ana veri dizini
        """
        self.base_dir = Path(base_dir)
        self.raw_audio_dir = self.base_dir / "raw_audio" / "good_sounds"
        self.processed_dir = self.base_dir / "processed"
        self.features_dir = self.processed_dir / "features"
        self.datasets_dir = self.processed_dir / "datasets"
        self.models_dir = Path("models")

        # Good Sounds veri setindeki hedef enstrÃ¼manlar
        self.target_instruments = ["cello", "clarinet", "flute", "trumpet", "violin"]

        # Model training iÃ§in gerekli config
        self.training_config = {}

        # YAMNet model cache
        self.yamnet_model = None

        self._create_directory_structure()
        print(f"ğŸ“ DataManager hazÄ±r - Ana dizin: {self.base_dir}")
        print(f"ğŸµ Good Sounds veri seti iÃ§in yapÄ±landÄ±rÄ±ldÄ±")
        print(f"ğŸ“‚ Raw audio dizini: {self.raw_audio_dir}")

    def _create_directory_structure(self):
        """Proje dizin yapÄ±sÄ±nÄ± oluÅŸturur"""
        directories = [
            self.base_dir,
            self.raw_audio_dir,
            self.processed_dir,
            self.features_dir,
            self.datasets_dir,
            self.processed_dir / "cv_splits",
            self.models_dir,
        ]

        # Good Sounds enstrÃ¼man dizinleri
        for instrument in self.target_instruments:
            directories.append(self.raw_audio_dir / instrument)

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)

        print(f"âœ… Good Sounds iÃ§in dizin yapÄ±sÄ± oluÅŸturuldu")

    def load_yamnet_model(self):
        """YAMNet modelini yÃ¼kler (cache iÃ§in) - GeliÅŸtirilmiÅŸ versiyon"""
        if self.yamnet_model is None:
            try:
                print("ğŸ“¥ YAMNet modeli yÃ¼kleniyor...")

                # ğŸ”§ Ã‡Ã–ZÃœM 1: Cache'i temizle
                import tempfile

                cache_dir = os.path.join(tempfile.gettempdir(), "tfhub_modules")

                try:
                    if os.path.exists(cache_dir):
                        print("ğŸ—‘ï¸  TensorFlow Hub cache temizleniyor...")
                        shutil.rmtree(cache_dir)
                        print("âœ… Cache temizlendi")
                except Exception as e:
                    print(f"âš ï¸  Cache temizleme uyarÄ±sÄ± (devam ediliyor): {e}")

                # ğŸ”§ Ã‡Ã–ZÃœM 2: Alternatif YAMNet URL'leri
                yamnet_urls = [
                    "https://tfhub.dev/google/yamnet/1",  # Orijinal
                    "https://www.kaggle.com/models/google/yamnet/TensorFlow2/yamnet/1",  # Alternatif 1
                ]

                model_loaded = False
                for i, url in enumerate(yamnet_urls):
                    try:
                        print(f"ğŸ”„ Deneme {i+1}/{len(yamnet_urls)}: {url}")
                        self.yamnet_model = hub.load(url)
                        model_loaded = True
                        print(f"âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
                        break
                    except Exception as e:
                        print(f"âŒ Bu URL baÅŸarÄ±sÄ±z: {str(e)[:100]}")
                        if i < len(yamnet_urls) - 1:
                            print("ğŸ”„ Alternatif URL deneniyor...")
                        continue

                if not model_loaded:
                    print("\n" + "=" * 70)
                    print("âŒ YAMNet otomatik yÃ¼kleme baÅŸarÄ±sÄ±z!")
                    print("=" * 70)
                    print("\nğŸ”§ MANUEL Ã‡Ã–ZÃœMLER:")
                    print("\n1ï¸âƒ£  TensorFlow sÃ¼rÃ¼mÃ¼nÃ¼zÃ¼ kontrol edin:")
                    print(
                        '   python -c "import tensorflow as tf; print(tf.__version__)"'
                    )
                    print("   Ã–nerilen: TensorFlow 2.10 veya Ã¼zeri")
                    print("\n2ï¸âƒ£  TensorFlow ve TensorFlow Hub'Ä± gÃ¼ncelleyin:")
                    print("   pip install --upgrade tensorflow tensorflow-hub")
                    print("\n3ï¸âƒ£  Manuel model indirme:")
                    print("   - https://tfhub.dev/google/yamnet/1")
                    print("   - Ä°ndirilen modeli yerel yoldan yÃ¼kleyin")
                    print("\n4ï¸âƒ£  Alternatif: Traditional Features kullanÄ±n")
                    print("   - MenÃ¼den '2' seÃ§eneÄŸini seÃ§in")
                    print("=" * 70)
                    return False

                # ğŸ§ª Model testi
                print("ğŸ§ª Model testi yapÄ±lÄ±yor...")
                test_audio = np.zeros(16000, dtype=np.float32)  # 1 saniye sessizlik
                _, embeddings, _ = self.yamnet_model(test_audio)
                print(f"âœ… Test baÅŸarÄ±lÄ±! Embedding shape: {embeddings.shape}")
                print(f"   Embedding dim: {embeddings.shape[-1]}")

                return True

            except Exception as e:
                print(f"\nâŒ YAMNet yÃ¼kleme hatasÄ±: {e}")
                print(
                    "\nğŸ’¡ Ã–NERÄ°: Traditional Features dataset kullanmayÄ± deneyin (seÃ§enek 2)"
                )
                return False

        return True

    def scan_audio_files(self) -> Dict[str, List[str]]:
        """
        Good Sounds veri setindeki ses dosyalarÄ±nÄ± tarar ve organize eder

        Returns:
            Dict: EnstrÃ¼man -> dosya listesi mapping
        """
        audio_files = {}
        supported_formats = {".wav", ".mp3", ".flac", ".m4a", ".aiff", ".au"}

        print(f"ğŸ” Good Sounds veri seti taranÄ±yor...")
        print(f"ğŸ“‚ Tarama dizini: {self.raw_audio_dir}")

        for instrument in self.target_instruments:
            instrument_dir = self.raw_audio_dir / instrument
            files = []

            if instrument_dir.exists():
                print(f"  ğŸ“ {instrument} dizini kontrol ediliyor: {instrument_dir}")
                for file_path in instrument_dir.rglob("*"):
                    if (
                        file_path.suffix.lower() in supported_formats
                        and file_path.is_file()
                    ):
                        files.append(str(file_path))
            else:
                print(f"  âš ï¸ {instrument} dizini bulunamadÄ±: {instrument_dir}")

            audio_files[instrument] = files
            print(f"  {instrument}: {len(files)} dosya")

        total_files = sum(len(files) for files in audio_files.values())
        print(f"ğŸ“Š Toplam {total_files} ses dosyasÄ± bulundu")

        return audio_files

    def validate_audio_files(
        self, audio_files: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """
        Ses dosyalarÄ±nÄ± validate eder, bozuk dosyalarÄ± filtreler

        Args:
            audio_files: TaranmÄ±ÅŸ ses dosyalarÄ±

        Returns:
            Dict: Validate edilmiÅŸ ses dosyalarÄ±
        """
        print("ğŸ” Good Sounds veri seti validate ediliyor...")

        validated_files = {}
        total_removed = 0

        for instrument, file_list in audio_files.items():
            valid_files = []

            print(f"  ğŸµ {instrument} enstrÃ¼manÄ± validate ediliyor...")

            for file_path in file_list:
                try:
                    # DosyayÄ± yÃ¼klemeyi dene
                    audio, sr = librosa.load(file_path, duration=1.0)

                    # Minimum sÃ¼re kontrolÃ¼ (0.5 saniye)
                    if len(audio) / sr >= 0.5:
                        valid_files.append(file_path)
                    else:
                        print(f"    âš ï¸ Ã‡ok kÄ±sa dosya atlandÄ±: {Path(file_path).name}")
                        total_removed += 1

                except Exception as e:
                    print(
                        f"    âš ï¸ Bozuk dosya atlandÄ±: {Path(file_path).name} - {str(e)[:50]}..."
                    )
                    total_removed += 1

            validated_files[instrument] = valid_files
            print(
                f"    âœ… {instrument}: {len(valid_files)}/{len(file_list)} geÃ§erli dosya"
            )

        if total_removed > 0:
            print(f"ğŸ“Š Toplam {total_removed} dosya filtrelendi")
        else:
            print(f"âœ… TÃ¼m dosyalar geÃ§erli")

        return validated_files

    def analyze_class_balance(self, labels: List[str]) -> Dict[str, Any]:
        """
        SÄ±nÄ±f dengesi analizi yapar

        Args:
            labels: Etiket listesi

        Returns:
            Dict: Denge analizi sonuÃ§larÄ±
        """
        label_counts = Counter(labels)
        min_count = min(label_counts.values())
        max_count = max(label_counts.values())
        imbalance_ratio = max_count / min_count if min_count > 0 else float("inf")

        analysis = {
            "label_counts": dict(label_counts),
            "min_count": min_count,
            "max_count": max_count,
            "imbalance_ratio": imbalance_ratio,
            "needs_balancing": imbalance_ratio > 2.0,
            "recommended_technique": (
                "class_weights" if imbalance_ratio > 2.0 else "none"
            ),
        }

        print(f"ğŸ“Š Good Sounds veri seti sÄ±nÄ±f dengesi analizi:")
        for instrument, count in label_counts.items():
            print(f"  {instrument}: {count} Ã¶rnek")
        print(f"  Dengesizlik oranÄ±: {imbalance_ratio:.2f}:1")
        if analysis["needs_balancing"]:
            print(f"  ğŸ”„ Ã–nerilen dengeleme: {analysis['recommended_technique']}")
        else:
            print(f"  âœ… Veri seti dengeli")

        return analysis

    def augment_audio_data(
        self,
        audio_data: np.ndarray,
        sample_rate: int = 16000,
        augmentation_factor: int = 3,
    ) -> List[np.ndarray]:
        """
        Good Sounds iÃ§in optimize edilmiÅŸ ses verisi artÄ±rÄ±mÄ±

        Args:
            audio_data: Orijinal ses verisi
            sample_rate: Ã–rnekleme frekansÄ± (16000 veya 22050)
            augmentation_factor: KaÃ§ adet augmented veri Ã¼retilecek

        Returns:
            List: ArtÄ±rÄ±lmÄ±ÅŸ ses verileri
        """
        augmented_data = [audio_data]  # Orijinal dahil

        try:
            # âœ… Sample rate'i lambda iÃ§inde kullan (closure ile)
            techniques = [
                # Pitch shifting
                lambda x, sr=sample_rate: librosa.effects.pitch_shift(
                    x, sr=sr, n_steps=1
                ),
                lambda x, sr=sample_rate: librosa.effects.pitch_shift(
                    x, sr=sr, n_steps=-1
                ),
                # Time stretching
                lambda x, sr=sample_rate: librosa.effects.time_stretch(x, rate=0.95),
                lambda x, sr=sample_rate: librosa.effects.time_stretch(x, rate=1.05),
                # Hafif gÃ¼rÃ¼ltÃ¼
                lambda x, sr=sample_rate: x + np.random.normal(0, 0.003, len(x)),
                # Gain variation
                lambda x, sr=sample_rate: x * np.random.uniform(0.8, 1.2),
            ]

            selected_techniques = techniques[
                : min(augmentation_factor, len(techniques))
            ]

            for i, technique in enumerate(selected_techniques):
                try:
                    augmented = technique(audio_data)
                    augmented = np.clip(augmented, -1.0, 1.0)
                    augmented_data.append(augmented)
                except Exception as e:
                    print(f"    âš ï¸ Augmentation tekniÄŸi {i+1} hatasÄ±: {e}")

        except Exception as e:
            print(f"âš ï¸ Genel augmentation hatasÄ±: {e}")

        return augmented_data

    def create_both_datasets(
        self,
        use_augmentation: bool = True,
        yamnet_augmentation_factor: int = 2,
        features_augmentation_factor: int = 3,
    ) -> Dict[str, Optional[str]]:
        """
        Hem YAMNet hem de Traditional Features iÃ§in dataset oluÅŸturur

        Args:
            use_augmentation: Augmentation kullanÄ±lsÄ±n mÄ±
            yamnet_augmentation_factor: YAMNet iÃ§in augmentation faktÃ¶rÃ¼
            features_augmentation_factor: Features iÃ§in augmentation faktÃ¶rÃ¼

        Returns:
            Dict: Her iki dataset'in dosya yollarÄ±
        """
        print("=" * 70)
        print("ğŸ¼ GOOD SOUNDS - Ä°KÄ° DATASET OLUÅTURMA")
        print("=" * 70)

        results = {}

        # 1ï¸âƒ£ YAMNet Dataset (16kHz)
        print("\n" + "=" * 70)
        print("1ï¸âƒ£ YAMNet Embeddings Dataset (16kHz)")
        print("=" * 70)

        yamnet_path = self.create_yamnet_dataset(
            output_name="good_sounds_yamnet.pkl",
            use_augmentation=use_augmentation,
            augmentation_factor=yamnet_augmentation_factor,
        )

        if yamnet_path:
            # YAMNet dataset'i bÃ¶l
            print("\nğŸ“Š YAMNet dataset bÃ¶lÃ¼nÃ¼yor...")
            yamnet_splits = self.split_dataset(
                yamnet_path, test_size=0.2, val_size=0.1, create_cv=False
            )

            results["yamnet"] = {
                "full_dataset": yamnet_path,
                "splits": yamnet_splits,
                "sample_rate": 16000,
                "data_type": "yamnet_embeddings",
            }

            print(f"âœ… YAMNet dataset hazÄ±r!")
            print(f"   ğŸ“ Full: {yamnet_path}")
            print(f"   ğŸ“ Train: {yamnet_splits['train']}")
            print(f"   ğŸ“ Val: {yamnet_splits['val']}")
            print(f"   ğŸ“ Test: {yamnet_splits['test']}")
        else:
            print("âŒ YAMNet dataset oluÅŸturulamadÄ±!")
            results["yamnet"] = None

        # 2ï¸âƒ£ Traditional Features Dataset (22kHz)
        print("\n" + "=" * 70)
        print("2ï¸âƒ£ Traditional Features Dataset (22kHz)")
        print("=" * 70)

        features_path = self.create_feature_dataset(
            output_name="good_sounds_features.pkl",
            use_augmentation=use_augmentation,
            augmentation_factor=features_augmentation_factor,
        )

        if features_path:
            # Features dataset'i bÃ¶l
            print("\nğŸ“Š Features dataset bÃ¶lÃ¼nÃ¼yor...")
            features_splits = self.split_dataset(
                features_path, test_size=0.2, val_size=0.1, create_cv=False
            )

            results["features"] = {
                "full_dataset": features_path,
                "splits": features_splits,
                "sample_rate": 22050,
                "data_type": "traditional_features",
            }

            print(f"âœ… Traditional Features dataset hazÄ±r!")
            print(f"   ğŸ“ Full: {features_path}")
            print(f"   ğŸ“ Train: {features_splits['train']}")
            print(f"   ğŸ“ Val: {features_splits['val']}")
            print(f"   ğŸ“ Test: {features_splits['test']}")
        else:
            print("âŒ Traditional Features dataset oluÅŸturulamadÄ±!")
            results["features"] = None

        # ğŸ“Š Ã–zet
        print("\n" + "=" * 70)
        print("ğŸ“Š DATASET OLUÅTURMA Ã–ZETÄ°")
        print("=" * 70)

        if results.get("yamnet"):
            print(f"\nâœ… YAMNet Dataset:")
            print(f"   Sample Rate: 16000 Hz")
            print(f"   Data Type: Embeddings (1024-dim)")
            print(f"   Full Dataset: {results['yamnet']['full_dataset']}")
        else:
            print(f"\nâŒ YAMNet Dataset oluÅŸturulamadÄ±")

        if results.get("features"):
            print(f"\nâœ… Traditional Features Dataset:")
            print(f"   Sample Rate: 22050 Hz")
            print(f"   Data Type: Handcrafted Features")
            print(f"   Full Dataset: {results['features']['full_dataset']}")
        else:
            print(f"\nâŒ Traditional Features Dataset oluÅŸturulamadÄ±")

        print("\n" + "=" * 70)

        return results

    def create_yamnet_dataset(
        self,
        output_name: str = "good_sounds_yamnet.pkl",
        use_augmentation: bool = True,
        augmentation_factor: int = 2,
        batch_size: int = 16,
    ) -> Optional[str]:
        """
        YAMNet embeddings ile dataset oluÅŸturur (Ã–NERILEN METOD)

        Args:
            output_name: Ã‡Ä±ktÄ± dosya adÄ±
            use_augmentation: Augmentation kullanÄ±lsÄ±n mÄ±
            augmentation_factor: Her dosya iÃ§in kaÃ§ augmentation
            batch_size: Batch boyutu

        Returns:
            str: OluÅŸturulan dataset dosya yolu
        """
        print("ğŸµ YAMNet embeddings dataset oluÅŸturuluyor...")
        print("âš¡ Bu metod CNN training iÃ§in optimize edilmiÅŸtir")

        # YAMNet modelini yÃ¼kle
        if not self.load_yamnet_model():
            print("âŒ YAMNet yÃ¼klenemedi!")
            return None

        # DosyalarÄ± tara ve validate et
        audio_files = self.scan_audio_files()
        validated_files = self.validate_audio_files(audio_files)

        all_embeddings = []
        all_labels = []
        file_info = []

        total_files = sum(len(files) for files in validated_files.values())

        if total_files == 0:
            print("âŒ GeÃ§erli ses dosyasÄ± bulunamadÄ±!")
            return None

        print(f"ğŸ”„ Augmentation: {'Aktif' if use_augmentation else 'Pasif'}")
        if use_augmentation:
            print(f"  FaktÃ¶r: x{augmentation_factor}")

        with tqdm(total=total_files, desc="YAMNet embeddings") as pbar:
            for instrument in self.target_instruments:
                file_list = validated_files.get(instrument, [])

                if not file_list:
                    print(f"âš ï¸ {instrument} iÃ§in dosya bulunamadÄ±")
                    continue

                for file_path in file_list:
                    try:
                        # 16kHz'de yÃ¼kle (YAMNet requirement)
                        waveform, _ = librosa.load(file_path, sr=16000, mono=True)

                        # Normalizasyon
                        waveform = waveform / (np.max(np.abs(waveform)) + 1e-8)

                        # Minimum uzunluk kontrolÃ¼
                        min_samples = int(0.96 * 16000)
                        if len(waveform) < min_samples:
                            waveform = np.pad(
                                waveform, (0, min_samples - len(waveform))
                            )

                        # Augmentation uygula
                        if use_augmentation:
                            audio_variants = self.augment_audio_data(
                                waveform, 16000, augmentation_factor
                            )
                        else:
                            audio_variants = [waveform]

                        # Her variant iÃ§in embedding Ã§Ä±kar
                        for i, audio_variant in enumerate(audio_variants):
                            try:
                                # YAMNet ile embedding
                                _, embeddings, _ = self.yamnet_model(audio_variant)
                                # Frame-level embeddings'leri ortala
                                avg_embedding = tf.reduce_mean(
                                    embeddings, axis=0
                                ).numpy()

                                all_embeddings.append(avg_embedding)
                                all_labels.append(instrument)
                                file_info.append(
                                    {
                                        "file_path": file_path,
                                        "instrument": instrument,
                                        "augmentation_id": i,
                                        "is_original": i == 0,
                                        "dataset": "good_sounds",
                                        "embedding_type": "yamnet",
                                    }
                                )

                            except Exception as e:
                                print(
                                    f"âš ï¸ Embedding hatasÄ± ({Path(file_path).name}): {e}"
                                )

                        pbar.set_postfix(
                            {
                                "Instrument": instrument,
                                "Embeddings": len(all_embeddings),
                            }
                        )

                    except Exception as e:
                        print(f"âš ï¸ Audio load hatasÄ± ({Path(file_path).name}): {e}")

                    pbar.update(1)

        if not all_embeddings:
            print("âŒ HiÃ§ embedding Ã§Ä±karÄ±lamadÄ±!")
            return None

        # Class balance analizi
        balance_analysis = self.analyze_class_balance(all_labels)

        # Dataset'i oluÅŸtur
        dataset = {
            "embeddings": np.array(all_embeddings),  # YAMNet embeddings
            "labels": all_labels,
            "file_info": file_info,
            "instruments": self.target_instruments,
            "embedding_type": "yamnet",
            "embedding_dim": 1024,  # YAMNet embedding size
            "sample_rate": 16000,
            "total_samples": len(all_embeddings),
            "augmentation_used": use_augmentation,
            "augmentation_factor": augmentation_factor if use_augmentation else 0,
            "balance_analysis": balance_analysis,
            "dataset_name": "good_sounds",
            "created_date": pd.Timestamp.now().isoformat(),
        }

        # Dataset'i kaydet
        output_path = self.datasets_dir / output_name

        with open(output_path, "wb") as f:
            pickle.dump(dataset, f)

        # Metadata kaydet
        metadata_path = (
            self.features_dir / f"metadata_{output_name.replace('.pkl', '.json')}"
        )
        metadata = {
            "embedding_type": "yamnet",
            "embedding_dim": 1024,
            "sample_count": len(all_embeddings),
            "instruments": self.target_instruments,
            "class_distribution": balance_analysis["label_counts"],
            "augmentation_info": {
                "used": use_augmentation,
                "factor": augmentation_factor,
            },
            "created_date": pd.Timestamp.now().isoformat(),
        }

        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)

        # Training config'i gÃ¼ncelle
        self.training_config.update(
            {
                "dataset_path": str(output_path),
                "metadata_path": str(metadata_path),
                "total_samples": len(all_embeddings),
                "embedding_dim": 1024,
                "embedding_type": "yamnet",
                "class_info": balance_analysis,
                "augmentation_info": {
                    "used": use_augmentation,
                    "factor": augmentation_factor,
                },
            }
        )

        print(f"âœ… YAMNet dataset oluÅŸturuldu: {output_path}")
        print(f"ğŸ“Š Toplam Ã¶rnek: {len(all_embeddings)}")
        print(f"ğŸ¼ EnstrÃ¼man daÄŸÄ±lÄ±mÄ±:")

        for instrument, count in balance_analysis["label_counts"].items():
            print(f"  {instrument}: {count} Ã¶rnek")

        print(f"ğŸ“ Metadata: {metadata_path}")

        return str(output_path)

    def create_feature_dataset(
        self,
        output_name: str = "good_sounds_features.pkl",
        use_augmentation: bool = True,
        augmentation_factor: int = 3,
        batch_size: int = 50,
    ) -> Optional[str]:
        """
        Good Sounds veri setinden traditional feature dataset oluÅŸturur
        NOT: YAMNet kullanÄ±yorsanÄ±z create_yamnet_dataset() metodunu kullanÄ±n

        Args:
            output_name: Ã‡Ä±ktÄ± dosya adÄ±
            use_augmentation: Augmentation kullanÄ±lsÄ±n mÄ±
            augmentation_factor: Her dosya iÃ§in kaÃ§ augmentation
            batch_size: Memory optimization iÃ§in batch boyutu

        Returns:
            str: OluÅŸturulan dataset dosya yolu
        """
        print("ğŸµ Traditional features dataset oluÅŸturuluyor...")
        print("âš ï¸  YAMNet kullanÄ±yorsanÄ±z create_yamnet_dataset() metodunu kullanÄ±n")

        # ModÃ¼lleri import et
        try:
            from src.audio_processing.classification_processor import (
                ClassificationProcessor,
            )
            from src.feature_extraction.classification_extractor import (
                ClassificationExtractor,
            )

            print("âœ… Classification processing modÃ¼lleri baÅŸarÄ±yla import edildi")
        except ImportError as e:
            print(f"âŒ Import hatasÄ±: {e}")
            return None

        # Processor'larÄ± baÅŸlat
        audio_processor = ClassificationProcessor(sample_rate=22050)  # 22kHz
        feature_extractor = ClassificationExtractor(sample_rate=22050)  # 22kHz

        # DosyalarÄ± tara ve validate et
        audio_files = self.scan_audio_files()
        validated_files = self.validate_audio_files(audio_files)

        all_features = []
        all_labels = []
        file_info = []

        total_files = sum(len(files) for files in validated_files.values())

        if total_files == 0:
            print("âŒ GeÃ§erli ses dosyasÄ± bulunamadÄ±!")
            return None

        print(f"ğŸ”„ Augmentation: {'Aktif' if use_augmentation else 'Pasif'}")

        batch_features = []
        batch_labels = []
        batch_info = []

        with tqdm(total=total_files, desc="Feature extraction") as pbar:
            for instrument in self.target_instruments:
                file_list = validated_files.get(instrument, [])

                if not file_list:
                    continue

                for file_path in file_list:
                    try:
                        audio_data, metadata = audio_processor.load_audio(file_path)
                        processed_audio = audio_processor.preprocess(audio_data)

                        if use_augmentation:
                            audio_variants = self.augment_audio_data(
                                processed_audio,
                                sample_rate=22050,  # âœ… Sabit deÄŸer
                                augmentation_factor=augmentation_factor,
                            )
                        else:
                            audio_variants = [processed_audio]

                        for i, audio_variant in enumerate(audio_variants):
                            features = feature_extractor.extract_features(audio_variant)

                            batch_features.append(features)
                            batch_labels.append(instrument)
                            batch_info.append(
                                {
                                    "file_path": file_path,
                                    "instrument": instrument,
                                    "augmentation_id": i,
                                    "is_original": i == 0,
                                    "dataset": "good_sounds",
                                }
                            )

                        if len(batch_features) >= batch_size:
                            all_features.extend(batch_features)
                            all_labels.extend(batch_labels)
                            file_info.extend(batch_info)

                            batch_features = []
                            batch_labels = []
                            batch_info = []

                    except Exception as e:
                        print(f"âš ï¸ Hata: {e}")

                    pbar.update(1)

        if batch_features:
            all_features.extend(batch_features)
            all_labels.extend(batch_labels)
            file_info.extend(batch_info)

        if not all_features:
            print("âŒ HiÃ§ Ã¶zellik Ã§Ä±karÄ±lamadÄ±!")
            return None

        balance_analysis = self.analyze_class_balance(all_labels)

        dataset = {
            "features": all_features,
            "labels": all_labels,
            "file_info": file_info,
            "instruments": self.target_instruments,
            "total_samples": len(all_features),
            "feature_type": "traditional",
            "augmentation_used": use_augmentation,
            "augmentation_factor": augmentation_factor if use_augmentation else 0,
            "balance_analysis": balance_analysis,
            "dataset_name": "good_sounds",
            "created_date": pd.Timestamp.now().isoformat(),
        }

        output_path = self.datasets_dir / output_name

        with open(output_path, "wb") as f:
            pickle.dump(dataset, f)

        print(f"âœ… Traditional features dataset oluÅŸturuldu: {output_path}")
        print(f"ğŸ“Š Toplam Ã¶rnek: {len(all_features)}")

        return str(output_path)

    def split_dataset(
        self,
        dataset_path: str,
        test_size: float = 0.2,
        val_size: float = 0.1,
        create_cv: bool = False,
    ) -> Dict[str, str]:
        """Dataset'i train/val/test olarak bÃ¶ler"""

        print(f"ğŸ“Š Dataset bÃ¶lÃ¼nÃ¼yor: test={test_size}, val={val_size}")

        with open(dataset_path, "rb") as f:
            dataset = pickle.load(f)

        # Embedding veya feature'larÄ± al
        if "embeddings" in dataset:
            X = dataset["embeddings"]
            data_key = "embeddings"
        elif "features" in dataset:
            X = dataset["features"]
            data_key = "features"
        else:
            print("âŒ Dataset'te embeddings veya features bulunamadÄ±!")
            return {}

        labels = dataset["labels"]
        file_info = dataset["file_info"]

        # Stratified split
        X_temp, X_test, y_temp, y_test, info_temp, info_test = train_test_split(
            X, labels, file_info, test_size=test_size, random_state=42, stratify=labels
        )

        val_size_adjusted = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val, info_train, info_val = train_test_split(
            X_temp,
            y_temp,
            info_temp,
            test_size=val_size_adjusted,
            random_state=42,
            stratify=y_temp,
        )

        # Splits oluÅŸtur
        splits = {
            "train": {
                data_key: X_train,
                "labels": y_train,
                "file_info": info_train,
                "split": "train",
                "instruments": self.target_instruments,
                "dataset_name": "good_sounds",
                "embedding_type": dataset.get("embedding_type", "unknown"),  # âœ… EKLE
                "embedding_dim": dataset.get("embedding_dim", None),  # âœ… EKLE
            },
            "val": {
                data_key: X_val,
                "labels": y_val,
                "file_info": info_val,
                "split": "validation",
                "instruments": self.target_instruments,
                "dataset_name": "good_sounds",
                "embedding_type": dataset.get("embedding_type", "unknown"),  # âœ… EKLE
                "embedding_dim": dataset.get("embedding_dim", None),  # âœ… EKLE
            },
            "test": {
                data_key: X_test,
                "labels": y_test,
                "file_info": info_test,
                "split": "test",
                "instruments": self.target_instruments,
                "dataset_name": "good_sounds",
                "embedding_type": dataset.get("embedding_type", "unknown"),  # âœ… EKLE
                "embedding_dim": dataset.get("embedding_dim", None),  # âœ… EKLE
            },
        }

        saved_paths = {}
        for split_name, split_data in splits.items():
            # âœ… DÃœZELTÄ°LMÄ°Å: Orijinal dataset adÄ±ndan tip bilgisini al
            original_filename = Path(
                dataset_path
            ).stem  # "good_sounds_yamnet" veya "good_sounds_features"
            split_path = (
                self.datasets_dir / f"{original_filename}_{split_name}_dataset.pkl"
            )

            with open(split_path, "wb") as f:
                pickle.dump(split_data, f)
            saved_paths[split_name] = str(split_path)

            print(f"  {split_name}: {len(split_data[data_key])} Ã¶rnek -> {split_path}")

        # Cross-validation splits
        if create_cv:
            cv_paths = self.create_cv_splits(X_train, y_train, info_train, data_key)
            saved_paths["cv_splits"] = cv_paths

        return saved_paths

    def create_cv_splits(
        self, X_train, y_train, info_train, data_key: str, k_folds: int = 5
    ) -> List[str]:
        """Cross-validation splits oluÅŸturur"""
        print(f"ğŸ”„ {k_folds}-fold CV splits oluÅŸturuluyor...")

        skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
        cv_paths = []

        cv_dir = self.processed_dir / "cv_splits"
        cv_dir.mkdir(exist_ok=True)

        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
            fold_data = {
                "fold_number": fold,
                f"train_{data_key}": [X_train[i] for i in train_idx],
                "train_labels": [y_train[i] for i in train_idx],
                "train_info": [info_train[i] for i in train_idx],
                f"val_{data_key}": [X_train[i] for i in val_idx],
                "val_labels": [y_train[i] for i in val_idx],
                "val_info": [info_train[i] for i in val_idx],
                "instruments": self.target_instruments,
                "dataset_name": "good_sounds",
            }

            fold_path = cv_dir / f"good_sounds_fold_{fold}.pkl"
            with open(fold_path, "wb") as f:
                pickle.dump(fold_data, f)

            cv_paths.append(str(fold_path))
            print(f"  Fold {fold}: {len(train_idx)} train, {len(val_idx)} val")

        return cv_paths

    def get_training_config(self) -> Dict[str, Any]:
        """Training konfigÃ¼rasyonu dÃ¶ndÃ¼rÃ¼r"""
        config = self.training_config.copy()
        config.update(
            {
                "instruments": self.target_instruments,
                "num_classes": len(self.target_instruments),
                "dataset_name": "good_sounds",
                "recommendations": {
                    "use_class_balancing": config.get("class_info", {}).get(
                        "needs_balancing", False
                    ),
                    "balancing_method": config.get("class_info", {}).get(
                        "recommended_technique", "class_weights"
                    ),
                    "suggested_epochs": 50,
                    "suggested_batch_size": 32,
                    "suggested_lr": 0.001,
                    "model_type": "yamnet_fine_tuning",
                    "early_stopping_patience": 10,
                },
            }
        )

        return config

    def get_dataset_info(self, dataset_path: str) -> Dict[str, Any]:
        """Dataset bilgilerini dÃ¶ndÃ¼rÃ¼r"""
        try:
            with open(dataset_path, "rb") as f:
                dataset = pickle.load(f)

            info = {
                "dataset_name": dataset.get("dataset_name", "unknown"),
                "total_samples": len(dataset.get("labels", [])),
                "instruments": dataset.get("instruments", []),
                "class_distribution": dict(Counter(dataset.get("labels", []))),
                "created_date": dataset.get("created_date", "unknown"),
            }

            if "embeddings" in dataset:
                info["data_type"] = "yamnet_embeddings"
                info["embedding_dim"] = dataset.get("embedding_dim", 1024)
            elif "features" in dataset:
                info["data_type"] = "traditional_features"
                info["feature_dim"] = (
                    len(dataset["features"][0]) if dataset["features"] else 0
                )

            return info

        except Exception as e:
            return {"error": str(e)}

    def list_datasets(self) -> List[Dict[str, Any]]:
        """Mevcut dataset'leri listeler"""
        datasets = []

        for pkl_file in self.datasets_dir.glob("*.pkl"):
            info = self.get_dataset_info(str(pkl_file))
            info["file_path"] = str(pkl_file)
            info["file_name"] = pkl_file.name
            info["file_size_mb"] = pkl_file.stat().st_size / (1024 * 1024)
            datasets.append(info)

        return datasets


def main():
    """Test ve demo fonksiyonu"""
    print("ğŸ¼ GOOD SOUNDS DATA MANAGER - Dual Dataset Edition")
    print("=" * 70)

    # DataManager oluÅŸtur
    dm = DataManager()

    # DosyalarÄ± kontrol et
    audio_files = dm.scan_audio_files()
    total_files = sum(len(files) for files in audio_files.values())

    if total_files == 0:
        print("âš ï¸ Ses dosyasÄ± bulunamadÄ±!")
        print(f"ğŸ“ {dm.raw_audio_dir}/ dizinine ses dosyalarÄ± ekleyin")
        return

    print(f"\nâœ… {total_files} ses dosyasÄ± bulundu!")

    # KullanÄ±cÄ±ya seÃ§enek sun
    print("\nğŸ¯ Dataset oluÅŸturma seÃ§enekleri:")
    print("  1ï¸âƒ£  Sadece YAMNet (16kHz embeddings)")
    print("  2ï¸âƒ£  Sadece Traditional Features (22kHz)")
    print("  3ï¸âƒ£  Her ikisi de (Ã–NERÄ°LEN)")

    choice = input("\nSeÃ§iminiz (1/2/3): ").strip()

    if choice == "1":
        # Sadece YAMNet
        print("\nğŸš€ YAMNet dataset oluÅŸturuluyor...")
        yamnet_path = dm.create_yamnet_dataset(
            output_name="good_sounds_yamnet.pkl",
            use_augmentation=True,
            augmentation_factor=2,
        )

        if yamnet_path:
            splits = dm.split_dataset(
                yamnet_path, test_size=0.2, val_size=0.1, create_cv=False
            )
            print(f"\nâœ… YAMNet dataset hazÄ±r!")
            print(f"ğŸ“ Train: {splits['train']}")
            print(f"ğŸ“ Val: {splits['val']}")
            print(f"ğŸ“ Test: {splits['test']}")

    elif choice == "2":
        # Sadece Traditional Features
        print("\nğŸŒ² Traditional Features dataset oluÅŸturuluyor...")
        features_path = dm.create_feature_dataset(
            output_name="good_sounds_features.pkl",
            use_augmentation=True,
            augmentation_factor=3,
        )

        if features_path:
            splits = dm.split_dataset(
                features_path, test_size=0.2, val_size=0.1, create_cv=False
            )
            print(f"\nâœ… Traditional Features dataset hazÄ±r!")
            print(f"ğŸ“ Train: {splits['train']}")
            print(f"ğŸ“ Val: {splits['val']}")
            print(f"ğŸ“ Test: {splits['test']}")

    elif choice == "3":
        # Her ikisi de
        print("\nğŸ¯ Her iki dataset de oluÅŸturuluyor...")
        results = dm.create_both_datasets(
            use_augmentation=True,
            yamnet_augmentation_factor=2,
            features_augmentation_factor=3,
        )

        print("\nâœ… Dataset oluÅŸturma tamamlandÄ±!")

        # DetaylÄ± bilgi
        if results.get("yamnet"):
            config_yamnet = dm.get_training_config()
            print(f"\nğŸ“Š YAMNet Training Config:")
            print(f"  Train samples: {config_yamnet['split_info']['train_samples']}")
            print(f"  Val samples: {config_yamnet['split_info']['val_samples']}")
            print(f"  Test samples: {config_yamnet['split_info']['test_samples']}")

    else:
        print("âŒ GeÃ§ersiz seÃ§im!")


if __name__ == "__main__":
    main()
