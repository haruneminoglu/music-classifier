"""
AutoML Classifier - AutoGluon ile otomatik model eÄŸitimi ve tahmin
DataManager YAMNet embeddings ve Traditional Features desteÄŸi
Proje yapÄ±sÄ±: src/ml_models/automl_classifier.py

Ã–zellikler:
- EÄŸitim modu: YAMNet veya Traditional Features ile model eÄŸitimi
- Tahmin modu: EÄŸitilmiÅŸ model ile yeni ses dosyalarÄ±nÄ± tahmin etme
"""

import pickle
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Tuple, Optional, Literal, List
from autogluon.tabular import TabularDataset, TabularPredictor
import sys
import os
import librosa
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Proje kÃ¶k dizinini sys.path'e ekle
current_dir = os.path.dirname(os.path.abspath(__file__))  # ml_models/
src_dir = os.path.dirname(current_dir)  # src/
project_root = os.path.dirname(src_dir)  # proje kÃ¶kÃ¼
sys.path.insert(0, project_root)


class AutoMLClassifier:
    """
    DataManager Ã§Ä±ktÄ±larÄ±nÄ± AutoGluon ile eÄŸiten classifier
    YAMNet embeddings ve Traditional Features iÃ§in optimize edilmiÅŸ
    """

    def __init__(self, project_root: str = None):
        """
        Args:
            project_root: Proje kÃ¶k dizini (None ise otomatik tespit)
        """
        if project_root is None:
            # Otomatik tespit: src/ml_models/ -> proje kÃ¶kÃ¼
            self.project_root = Path(__file__).parent.parent.parent
        else:
            self.project_root = Path(project_root)

        self.data_dir = self.project_root / "data" / "processed" / "datasets"
        self.models_dir = self.project_root / "models"

        # YAMNet model cache
        self.yamnet_model = None

        print(f"ğŸ¤– AutoMLClassifier hazÄ±r")
        print(f"ğŸ“ Proje kÃ¶kÃ¼: {self.project_root}")
        print(f"ğŸ“ Dataset dizini: {self.data_dir}")
        print(f"ğŸ“ Models dizini: {self.models_dir}")

    def load_dataset(self, pkl_path: str) -> Dict:
        """Pickle dataset'i yÃ¼kler ve tipini kontrol eder"""
        pkl_path = Path(pkl_path)

        if not pkl_path.exists():
            raise FileNotFoundError(f"âŒ Dataset bulunamadÄ±: {pkl_path}")

        with open(pkl_path, "rb") as f:
            dataset = pickle.load(f)

        # Dataset tipini kontrol et
        if "embeddings" in dataset:
            data_type = "yamnet"
            embedding_type = dataset.get("embedding_type", "unknown")
            print(f"âœ… YAMNet dataset yÃ¼klendi (type: {embedding_type})")
        elif "features" in dataset:
            data_type = "traditional"
            print(f"âœ… Traditional features dataset yÃ¼klendi")
        else:
            raise ValueError("âŒ Dataset'te 'embeddings' veya 'features' bulunamadÄ±!")

        dataset["_detected_type"] = data_type
        return dataset

    def convert_to_dataframe(
        self, dataset_dict: Dict, label_column: str = "instrument"
    ) -> pd.DataFrame:
        """
        DataManager formatÄ±nÄ± AutoGluon DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r

        Args:
            dataset_dict: DataManager pickle Ã§Ä±ktÄ±sÄ±
            label_column: Label sÃ¼tun ismi

        Returns:
            pd.DataFrame: AutoGluon uyumlu format
        """
        data_type = dataset_dict.get("_detected_type", "unknown")

        if data_type == "yamnet":
            # YAMNet embeddings (numpy array)
            data = dataset_dict["embeddings"]

            if not isinstance(data, np.ndarray):
                data = np.array(data)

            print(f"ğŸ“Š YAMNet embeddings dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
            print(f"   Shape: {data.shape}")
            print(f"   Dtype: {data.dtype}")

            # SÃ¼tun isimleri: embedding_0, embedding_1, ..., embedding_1023
            columns = [f"embedding_{i}" for i in range(data.shape[1])]

        elif data_type == "traditional":
            # Traditional features (list of dicts veya numpy array)
            features = dataset_dict["features"]

            if isinstance(features, list) and len(features) > 0:
                if isinstance(features[0], dict):
                    # List of dicts -> numpy array
                    data = np.array([list(f.values()) for f in features])
                    print(f"ğŸ“Š Traditional features (dict) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
                else:
                    # Zaten array/list
                    data = np.array(features)
                    print(f"ğŸ“Š Traditional features (array) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
            else:
                data = np.array(features)

            print(f"   Shape: {data.shape}")
            print(f"   Dtype: {data.dtype}")

            # SÃ¼tun isimleri: feature_0, feature_1, ...
            columns = [f"feature_{i}" for i in range(data.shape[1])]

        else:
            raise ValueError(f"âŒ Bilinmeyen data type: {data_type}")

        # Labels
        labels = dataset_dict["labels"]

        if len(data) != len(labels):
            raise ValueError(
                f"âŒ Data-label boyut uyumsuzluÄŸu: {len(data)} vs {len(labels)}"
            )

        # DataFrame oluÅŸtur
        df = pd.DataFrame(data, columns=columns)
        df[label_column] = labels

        print(f"âœ… DataFrame oluÅŸturuldu:")
        print(f"   SatÄ±rlar: {len(df)}")
        print(f"   Feature sÃ¼tunlar: {len(columns)}")
        print(f"   SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        print(df[label_column].value_counts().to_string())

        return df

    def prepare_datasets(
        self,
        dataset_type: Literal["yamnet", "traditional"],
        save_csv: bool = True,
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Train/Val/Test setlerini AutoGluon formatÄ±na hazÄ±rlar

        Args:
            dataset_type: "yamnet" veya "traditional"
            save_csv: CSV olarak kaydet mi?

        Returns:
            Tuple[train_df, val_df, test_df]
        """
        print("=" * 70)
        print(f"ğŸ”„ {dataset_type.upper()} veri setleri hazÄ±rlanÄ±yor...")
        print("=" * 70)

        # Dataset yollarÄ±nÄ± ayarla
        if dataset_type == "yamnet":
            # YAMNet iÃ§in 16kHz dataset'leri
            train_pkl = self.data_dir / "good_sounds_yamnet_train_dataset.pkl"
            val_pkl = self.data_dir / "good_sounds_yamnet_val_dataset.pkl"
            test_pkl = self.data_dir / "good_sounds_yamnet_test_dataset.pkl"
            csv_dir = self.project_root / "data" / "processed" / "autogluon_yamnet"

        elif dataset_type == "traditional":
            # Traditional features iÃ§in 22kHz dataset'leri
            train_pkl = self.data_dir / "good_sounds_features_train_dataset.pkl"
            val_pkl = self.data_dir / "good_sounds_features_val_dataset.pkl"
            test_pkl = self.data_dir / "good_sounds_features_test_dataset.pkl"
            csv_dir = self.project_root / "data" / "processed" / "autogluon_features"

        else:
            raise ValueError(f"âŒ GeÃ§ersiz dataset_type: {dataset_type}")

        # Dataset'leri yÃ¼kle
        print("\n1ï¸âƒ£  Dataset'ler yÃ¼kleniyor...")
        print(f"   Train: {train_pkl.name}")
        print(f"   Val: {val_pkl.name}")
        print(f"   Test: {test_pkl.name}")

        train_dict = self.load_dataset(str(train_pkl))
        val_dict = self.load_dataset(str(val_pkl))
        test_dict = self.load_dataset(str(test_pkl))

        # DataFrame'lere dÃ¶nÃ¼ÅŸtÃ¼r
        print("\n2ï¸âƒ£  DataFrame'lere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
        train_df = self.convert_to_dataframe(train_dict, "instrument")
        val_df = self.convert_to_dataframe(val_dict, "instrument")
        test_df = self.convert_to_dataframe(test_dict, "instrument")

        # CSV olarak kaydet (opsiyonel)
        if save_csv:
            print(f"\n3ï¸âƒ£  CSV dosyalarÄ± kaydediliyor...")
            csv_dir.mkdir(parents=True, exist_ok=True)

            train_csv = csv_dir / "train.csv"
            val_csv = csv_dir / "val.csv"
            test_csv = csv_dir / "test.csv"

            train_df.to_csv(train_csv, index=False)
            val_df.to_csv(val_csv, index=False)
            test_df.to_csv(test_csv, index=False)

            print(f"   âœ… Train: {train_csv}")
            print(f"   âœ… Val: {val_csv}")
            print(f"   âœ… Test: {test_csv}")

        print("\n" + "=" * 70)
        print("âœ… AutoGluon veri setleri hazÄ±r!")
        print("=" * 70)

        return train_df, val_df, test_df

    def train(
        self,
        train_df: pd.DataFrame,
        dataset_type: Literal["yamnet", "traditional"],
        label_column: str = "instrument",
        time_limit: int = 3600,
        eval_metric: str = "accuracy",
        presets: str = "best_quality",
    ) -> TabularPredictor:
        """
        AutoGluon ile model eÄŸitir

        Args:
            train_df: EÄŸitim DataFrame'i
            dataset_type: "yamnet" veya "traditional"
            label_column: Label sÃ¼tun ismi
            time_limit: Maksimum eÄŸitim sÃ¼resi (saniye)
            eval_metric: DeÄŸerlendirme metriÄŸi
            presets: AutoGluon preset

        Returns:
            TabularPredictor: EÄŸitilmiÅŸ model
        """
        print("=" * 70)
        print(f"ğŸ¤– AutoGluon Training - {dataset_type.upper()}")
        print("=" * 70)
        print(f"â±ï¸  Time limit: {time_limit}s ({time_limit/60:.1f} dakika)")
        print(f"ğŸ¯ Preset: {presets}")
        print(f"ğŸ“Š Eval metric: {eval_metric}")

        # Model kayÄ±t dizini
        output_dir = self.models_dir / f"autogluon_{dataset_type}"
        output_dir.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ“ Output dir: {output_dir}")

        # TabularPredictor oluÅŸtur
        predictor = TabularPredictor(
            label=label_column, eval_metric=eval_metric, path=str(output_dir)
        )

        # EÄŸitim
        print("\nğŸš€ Training baÅŸladÄ±...")
        predictor.fit(
            train_data=train_df,
            time_limit=time_limit,
            presets=presets,
            verbosity=2,  # DetaylÄ± log
        )

        print("\nâœ… Training tamamlandÄ±!")
        print(f"ğŸ“ Model kaydedildi: {output_dir}")

        return predictor

    def evaluate(
        self,
        predictor: TabularPredictor,
        test_df: pd.DataFrame,
        label_column: str = "instrument",
    ) -> Dict:
        """
        AutoGluon modelini deÄŸerlendirir

        Args:
            predictor: EÄŸitilmiÅŸ AutoGluon model
            test_df: Test DataFrame'i
            label_column: Label sÃ¼tun ismi

        Returns:
            Dict: DeÄŸerlendirme sonuÃ§larÄ±
        """
        print("=" * 70)
        print("ğŸ“Š Model DeÄŸerlendirme")
        print("=" * 70)

        # Test seti Ã¼zerinde tahmin
        y_pred = predictor.predict(test_df.drop(columns=[label_column]))
        y_true = test_df[label_column]

        # Performans metrikleri
        from sklearn.metrics import (
            accuracy_score,
            precision_recall_fscore_support,
            classification_report,
            confusion_matrix,
        )

        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average="weighted"
        )

        results = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "classification_report": classification_report(y_true, y_pred),
            "confusion_matrix": confusion_matrix(y_true, y_pred).tolist(),
            "y_true": y_true.tolist(),
            "y_pred": y_pred.tolist(),
        }

        print(f"\nğŸ“ˆ Test SonuÃ§larÄ±:")
        print(f"   Accuracy:  {accuracy:.4f}")
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall:    {recall:.4f}")
        print(f"   F1-Score:  {f1:.4f}")

        print(f"\nğŸ“‹ DetaylÄ± Rapor:")
        print(results["classification_report"])

        # Leaderboard
        print(f"\nğŸ† AutoGluon Model Leaderboard:")
        leaderboard = predictor.leaderboard(test_df, silent=True)
        print(leaderboard)

        return results

    def full_pipeline(
        self,
        dataset_type: Literal["yamnet", "traditional"],
        time_limit: int = 1800,
        presets: str = "medium_quality",
    ) -> Dict:
        """
        TEK KOMUTLA: DÃ¶nÃ¼ÅŸtÃ¼r + EÄŸit + DeÄŸerlendir

        Args:
            dataset_type: "yamnet" veya "traditional"
            time_limit: EÄŸitim sÃ¼resi (saniye)
            presets: "best_quality", "high_quality", "medium_quality"

        Returns:
            Dict: TÃ¼m sonuÃ§lar
        """
        print("ğŸš€ TAM PIPELINE - Tek Komut")
        print("=" * 70)

        # 1ï¸âƒ£ Veri setlerini hazÄ±rla
        train_df, val_df, test_df = self.prepare_datasets(
            dataset_type=dataset_type, save_csv=True
        )

        # Train + Val birleÅŸtir
        combined_train = pd.concat([train_df, val_df], ignore_index=True)
        print(f"\nğŸ”„ Train + Val birleÅŸtirildi: {len(combined_train)} Ã¶rnek")

        # 2ï¸âƒ£ Model eÄŸit
        predictor = self.train(
            train_df=combined_train,
            dataset_type=dataset_type,
            label_column="instrument",
            time_limit=time_limit,
            presets=presets,
        )

        # 3ï¸âƒ£ DeÄŸerlendir
        results = self.evaluate(predictor=predictor, test_df=test_df)

        # Model bilgilerini ekle
        model_path = self.models_dir / f"autogluon_{dataset_type}"

        output = {
            "predictor": predictor,
            "results": results,
            "model_path": str(model_path),
            "dataset_type": dataset_type,
            "test_accuracy": results["accuracy"],
            "test_f1": results["f1_score"],
        }

        # Ã–zet
        print("\n" + "=" * 70)
        print("âœ… PIPELINE TAMAMLANDI!")
        print("=" * 70)
        print(f"ğŸ“Š Dataset Type: {dataset_type}")
        print(f"ğŸ“Š Test Accuracy: {results['accuracy']:.4f}")
        print(f"ğŸ“Š Test F1-Score: {results['f1_score']:.4f}")
        print(f"ğŸ“ Model: {model_path}")

        return output

    def list_available_models(self) -> List[Dict[str, str]]:
        """EÄŸitilmiÅŸ modelleri listeler"""
        models = []

        if not self.models_dir.exists():
            print("âš ï¸ Models dizini bulunamadÄ±!")
            return models

        for model_dir in self.models_dir.iterdir():
            if model_dir.is_dir() and model_dir.name.startswith("autogluon_"):
                # Model tipini tespit et (iyileÅŸtirilmiÅŸ)
                if "yamnet" in model_dir.name:
                    model_type = "yamnet"
                elif "traditional" in model_dir.name or "features" in model_dir.name:
                    model_type = "traditional"
                else:
                    model_type = "unknown"

                models.append(
                    {
                        "name": model_dir.name,
                        "path": str(model_dir),
                        "type": model_type,
                    }
                )

        return models

    def load_yamnet_model(self):
        """YAMNet modelini yÃ¼kler (tahmin iÃ§in)"""
        if self.yamnet_model is None:
            try:
                print("ğŸ“¥ YAMNet modeli yÃ¼kleniyor...")
                self.yamnet_model = hub.load("https://tfhub.dev/google/yamnet/1")

                # Test
                test_audio = np.zeros(16000, dtype=np.float32)
                _, embeddings, _ = self.yamnet_model(test_audio)
                print(f"âœ… YAMNet yÃ¼klendi! Embedding shape: {embeddings.shape}")

                return True
            except Exception as e:
                print(f"âŒ YAMNet yÃ¼kleme hatasÄ±: {e}")
                return False
        return True

    def extract_yamnet_embedding(self, audio_path: str) -> np.ndarray:
        """
        Ses dosyasÄ±ndan YAMNet embedding Ã§Ä±karÄ±r

        Args:
            audio_path: Ses dosyasÄ± yolu

        Returns:
            np.ndarray: 1024-dim embedding
        """
        if not self.load_yamnet_model():
            raise RuntimeError("YAMNet modeli yÃ¼klenemedi!")

        # 16kHz'de yÃ¼kle
        waveform, _ = librosa.load(audio_path, sr=16000, mono=True)

        # Normalizasyon
        waveform = waveform / (np.max(np.abs(waveform)) + 1e-8)

        # Minimum uzunluk kontrolÃ¼
        min_samples = int(0.96 * 16000)
        if len(waveform) < min_samples:
            waveform = np.pad(waveform, (0, min_samples - len(waveform)))

        # YAMNet embedding
        _, embeddings, _ = self.yamnet_model(waveform)
        avg_embedding = tf.reduce_mean(embeddings, axis=0).numpy()

        return avg_embedding

    def extract_traditional_features(self, audio_path: str) -> np.ndarray:
        """
        Ses dosyasÄ±ndan traditional features Ã§Ä±karÄ±r

        Args:
            audio_path: Ses dosyasÄ± yolu

        Returns:
            np.ndarray: Feature vektÃ¶rÃ¼
        """
        try:
            from src.audio_processing.classification_processor import (
                ClassificationProcessor,
            )
            from src.feature_extraction.classification_extractor import (
                ClassificationExtractor,
            )
        except ImportError as e:
            raise ImportError(f"Feature extraction modÃ¼lleri yÃ¼klenemedi: {e}")

        # 22kHz'de yÃ¼kle
        audio_processor = ClassificationProcessor(sample_rate=22050)
        feature_extractor = ClassificationExtractor(sample_rate=22050)

        audio_data, _ = audio_processor.load_audio(audio_path)
        processed_audio = audio_processor.preprocess(audio_data)
        features = feature_extractor.extract_features(processed_audio)

        # Features'Ä±n tipini kontrol et ve uygun ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼r
        if isinstance(features, dict):
            # Dict -> array
            features_array = np.array(list(features.values()))
        elif isinstance(features, np.ndarray):
            # Zaten array ise dÃ¼zleÅŸtir
            features_array = features.flatten()
        elif isinstance(features, list):
            # List ise array'e Ã§evir
            features_array = np.array(features)
        else:
            raise TypeError(f"Beklenmeyen feature tipi: {type(features)}")

        # Debug: feature boyutunu gÃ¶ster
        print(f"   ğŸ“ Feature shape: {features_array.shape}")

        return features_array

    def predict_single_audio(
        self,
        audio_path: str,
        model_path: str,
        model_type: Literal["yamnet", "traditional"],
        verbose: bool = True,
    ) -> Dict[str, any]:
        """
        Tek ses dosyasÄ± iÃ§in tahmin yapar

        Args:
            audio_path: Ses dosyasÄ± yolu
            model_path: EÄŸitilmiÅŸ model dizini
            model_type: "yamnet" veya "traditional"
            verbose: DetaylÄ± Ã§Ä±ktÄ±

        Returns:
            Dict: Tahmin sonuÃ§larÄ±
        """
        audio_path = Path(audio_path)

        if not audio_path.exists():
            raise FileNotFoundError(f"âŒ Ses dosyasÄ± bulunamadÄ±: {audio_path}")

        if verbose:
            print(f"\nğŸµ Tahmin yapÄ±lÄ±yor: {audio_path.name}")

        # Feature extraction
        if model_type == "yamnet":
            if verbose:
                print("   ğŸ“Š YAMNet embedding Ã§Ä±karÄ±lÄ±yor...")
            features = self.extract_yamnet_embedding(str(audio_path))
            feature_names = [f"embedding_{i}" for i in range(len(features))]
        else:
            if verbose:
                print("   ğŸ“Š Traditional features Ã§Ä±karÄ±lÄ±yor...")
            features = self.extract_traditional_features(str(audio_path))
            feature_names = [f"feature_{i}" for i in range(len(features))]

        # DataFrame oluÅŸtur
        df = pd.DataFrame([features], columns=feature_names)

        # Model yÃ¼kle ve tahmin yap
        if verbose:
            print("   ğŸ¤– Model yÃ¼kleniyor...")
        predictor = TabularPredictor.load(model_path)

        prediction = predictor.predict(df)[0]
        probabilities = predictor.predict_proba(df).iloc[0].to_dict()

        result = {
            "file": audio_path.name,
            "prediction": prediction,
            "probabilities": probabilities,
            "confidence": probabilities[prediction],
        }

        if verbose:
            print(f"   âœ… Tahmin: {prediction} (gÃ¼ven: {result['confidence']:.2%})")
            print(f"   ğŸ“Š OlasÄ±lÄ±klar:")
            for instrument, prob in sorted(
                probabilities.items(), key=lambda x: x[1], reverse=True
            ):
                print(f"      {instrument}: {prob:.2%}")

        return result

    def batch_predict(
        self,
        audio_dir: str,
        model_path: str,
        model_type: Literal["yamnet", "traditional"],
        ground_truth_map: Optional[Dict[str, str]] = None,
    ) -> Dict[str, any]:
        """
        KlasÃ¶rdeki tÃ¼m ses dosyalarÄ± iÃ§in toplu tahmin

        Args:
            audio_dir: Ses dosyalarÄ±nÄ±n bulunduÄŸu klasÃ¶r
            model_path: EÄŸitilmiÅŸ model dizini
            model_type: "yamnet" veya "traditional"
            ground_truth_map: {dosya_adÄ±: gerÃ§ek_etiket} (opsiyonel)

        Returns:
            Dict: Toplu tahmin sonuÃ§larÄ±
        """
        audio_dir = Path(audio_dir)

        if not audio_dir.exists():
            raise FileNotFoundError(f"âŒ KlasÃ¶r bulunamadÄ±: {audio_dir}")

        # Ses dosyalarÄ±nÄ± bul
        audio_extensions = {".wav", ".mp3", ".flac", ".m4a", ".aiff"}
        audio_files = [
            f for f in audio_dir.rglob("*") if f.suffix.lower() in audio_extensions
        ]

        if not audio_files:
            print(f"âš ï¸ {audio_dir} iÃ§inde ses dosyasÄ± bulunamadÄ±!")
            return {}

        print(f"ğŸµ {len(audio_files)} ses dosyasÄ± bulundu")
        print(f"ğŸ¤– Model: {Path(model_path).name}")
        print(f"ğŸ“Š Tip: {model_type.upper()}")
        print("=" * 70)

        results = []
        y_true = []
        y_pred = []

        for i, audio_file in enumerate(audio_files, 1):
            print(f"\n[{i}/{len(audio_files)}] {audio_file.name}")

            try:
                result = self.predict_single_audio(
                    str(audio_file), model_path, model_type, verbose=False
                )

                results.append(result)
                y_pred.append(result["prediction"])

                # Ground truth varsa ekle
                if ground_truth_map and audio_file.name in ground_truth_map:
                    true_label = ground_truth_map[audio_file.name]
                    y_true.append(true_label)
                    is_correct = true_label == result["prediction"]
                    print(
                        f"   GerÃ§ek: {true_label} | Tahmin: {result['prediction']} | {'âœ…' if is_correct else 'âŒ'}"
                    )
                else:
                    print(
                        f"   Tahmin: {result['prediction']} (gÃ¼ven: {result['confidence']:.2%})"
                    )

            except Exception as e:
                print(f"   âš ï¸ Hata: {e}")

        # Ã–zet istatistikler
        summary = {
            "total_files": len(audio_files),
            "successful_predictions": len(results),
            "failed_predictions": len(audio_files) - len(results),
            "results": results,
        }

        # EÄŸer ground truth varsa deÄŸerlendirme yap
        if y_true and y_pred and len(y_true) == len(y_pred):
            accuracy = accuracy_score(y_true, y_pred)
            conf_matrix = confusion_matrix(y_true, y_pred)
            class_report = classification_report(y_true, y_pred)

            summary["evaluation"] = {
                "accuracy": accuracy,
                "confusion_matrix": conf_matrix.tolist(),
                "classification_report": class_report,
            }

            print("\n" + "=" * 70)
            print("ğŸ“Š DEÄERLENDÄ°RME SONUÃ‡LARI")
            print("=" * 70)
            print(f"âœ… DoÄŸruluk (Accuracy): {accuracy:.2%}")
            print(f"\nğŸ“‹ DetaylÄ± Rapor:")
            print(class_report)

        # Tahmin daÄŸÄ±lÄ±mÄ±
        from collections import Counter

        pred_dist = Counter(y_pred)

        print("\n" + "=" * 70)
        print("ğŸ“ˆ TAHMÄ°N DAÄILIMI")
        print("=" * 70)
        for instrument, count in pred_dist.most_common():
            print(f"{instrument}: {count} dosya ({count/len(y_pred)*100:.1f}%)")

        return summary


def main():
    """
    Ana program - MenÃ¼ sistemi ile eÄŸitim veya tahmin
    """
    print("ğŸ¼ AUTOML CLASSIFIER - EÄŸitim & Tahmin Sistemi")
    print("=" * 70)

    # Classifier oluÅŸtur
    classifier = AutoMLClassifier()

    print("\nğŸ¯ Ne yapmak istersiniz?")
    print("  1ï¸âƒ£  Model EÄŸitimi (Yeni model oluÅŸtur)")
    print("  2ï¸âƒ£  Tahmin (Mevcut model ile tahmin yap)")
    print("  3ï¸âƒ£  Ã‡Ä±kÄ±ÅŸ")

    mode = input("\nSeÃ§iminiz (1/2/3): ").strip()

    # ==================== EÄÄ°TÄ°M MODU ====================
    if mode == "1":
        print("\n" + "=" * 70)
        print("ğŸ“š EÄÄ°TÄ°M MODU")
        print("=" * 70)

        print("\nğŸ¯ Hangi dataset tipini kullanmak istersiniz?")
        print("  1ï¸âƒ£  YAMNet Embeddings (16kHz, 1024-dim) - Ã–NERÄ°LEN")
        print("  2ï¸âƒ£  Traditional Features (22kHz, handcrafted)")

        choice = input("\nSeÃ§iminiz (1/2): ").strip()

        dataset_type = "yamnet" if choice == "1" else "traditional"

        print(f"\nâœ… {dataset_type.upper()} seÃ§ildi")

        # KullanÄ±cÄ±ya zaman/kalite seÃ§eneÄŸi sun
        print("\nâ±ï¸ EÄŸitim sÃ¼resi ve kalite seÃ§enekleri:")
        print("  1ï¸âƒ£  HÄ±zlÄ± Deneme (10 dk) - ~%88-90 accuracy")
        print("  2ï¸âƒ£  Normal Kalite (30 dk) - ~%95-95.5 accuracy [Ã–NERÄ°LEN]")
        print("  3ï¸âƒ£  YÃ¼ksek Kalite (60 dk) - ~%95.5-96 accuracy")
        print("  4ï¸âƒ£  En Ä°yi Kalite (2 saat) - ~%96-96.5 accuracy")

        time_choice = (
            input("\nSÃ¼re seÃ§iminiz (1/2/3/4) [varsayÄ±lan: 2]: ").strip() or "2"
        )

        time_configs = {
            "1": {
                "time_limit": 600,
                "presets": "optimize_for_deployment",
                "name": "HÄ±zlÄ±",
            },
            "2": {"time_limit": 1800, "presets": "medium_quality", "name": "Normal"},
            "3": {"time_limit": 3600, "presets": "high_quality", "name": "YÃ¼ksek"},
            "4": {"time_limit": 7200, "presets": "best_quality", "name": "En Ä°yi"},
        }

        config = time_configs.get(time_choice, time_configs["2"])

        print(f"\nâœ… {config['name']} kalite seÃ§ildi")
        print(f"\nâš™ï¸ Training parametreleri:")
        print(
            f"  Time limit: {config['time_limit']} saniye ({config['time_limit']/60:.0f} dakika)"
        )
        print(f"  Preset: {config['presets']}")
        print(f"  Eval metric: accuracy")

        confirm = input("\nDevam etmek istiyor musunuz? (y/n): ").strip().lower()

        if confirm != "y":
            print("âŒ Ä°ÅŸlem iptal edildi")
            return

        # Tam pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
        result = classifier.full_pipeline(
            dataset_type=dataset_type,
            time_limit=config["time_limit"],
            presets=config["presets"],
        )

        print("\nğŸ‰ EÄŸitim baÅŸarÄ±yla tamamlandÄ±!")
        print(f"   ğŸ“ Model yolu: {result['model_path']}")
        print(f"   ğŸ“Š Test accuracy: {result['test_accuracy']:.4f}")
        print(f"   ğŸ“Š Test F1-score: {result['test_f1']:.4f}")

    # ==================== TAHMÄ°N MODU ====================
    elif mode == "2":
        print("\n" + "=" * 70)
        print("ğŸ”® TAHMÄ°N MODU")
        print("=" * 70)

        # Mevcut modelleri listele
        print("\nğŸ“‚ Mevcut eÄŸitilmiÅŸ modeller:")
        models = classifier.list_available_models()

        if not models:
            print("âŒ HiÃ§ eÄŸitilmiÅŸ model bulunamadÄ±!")
            print("ğŸ’¡ Ã–nce model eÄŸitimi yapmalÄ±sÄ±nÄ±z (SeÃ§enek 1)")
            return

        for i, model in enumerate(models, 1):
            print(f"  {i}ï¸âƒ£  {model['name']} ({model['type']})")

        # Model seÃ§
        model_choice = input(
            f"\nHangi modeli kullanmak istersiniz? (1-{len(models)}): "
        ).strip()

        try:
            model_idx = int(model_choice) - 1
            selected_model = models[model_idx]
            print(f"âœ… SeÃ§ilen model: {selected_model['name']}")
        except (ValueError, IndexError):
            print("âŒ GeÃ§ersiz seÃ§im!")
            return

        # Tahmin tipi seÃ§
        print("\nğŸµ Tahmin yapmak istediÄŸiniz seÃ§eneÄŸi seÃ§in:")
        print("  1ï¸âƒ£  Tek ses dosyasÄ±")
        print("  2ï¸âƒ£  KlasÃ¶rdeki tÃ¼m ses dosyalarÄ±")

        pred_type = input("\nSeÃ§iminiz (1/2): ").strip()

        if pred_type == "1":
            # Tek dosya tahmini
            audio_path = input("\nğŸ“ Ses dosyasÄ± yolunu girin: ").strip()

            if not Path(audio_path).exists():
                print(f"âŒ Dosya bulunamadÄ±: {audio_path}")
                return

            try:
                result = classifier.predict_single_audio(
                    audio_path=audio_path,
                    model_path=selected_model["path"],
                    model_type=selected_model["type"],
                    verbose=True,
                )

                print("\n" + "=" * 70)
                print("âœ… TAHMÄ°N TAMAMLANDI")
                print("=" * 70)
                print(f"ğŸ“ Dosya: {result['file']}")
                print(f"ğŸµ Tahmin: {result['prediction']}")
                print(f"ğŸ’¯ GÃ¼ven: {result['confidence']:.2%}")

            except Exception as e:
                print(f"âŒ Tahmin hatasÄ±: {e}")

        elif pred_type == "2":
            # Toplu tahmin
            audio_dir = input(
                "\nğŸ“ Ses dosyalarÄ±nÄ±n bulunduÄŸu klasÃ¶r yolunu girin: "
            ).strip()

            if not Path(audio_dir).exists():
                print(f"âŒ KlasÃ¶r bulunamadÄ±: {audio_dir}")
                return

            # Ground truth sorgusu
            use_gt = (
                input("\nâ“ Ground truth (gerÃ§ek etiketler) var mÄ±? (y/n): ")
                .strip()
                .lower()
            )

            ground_truth_map = None
            if use_gt == "y":
                print("\nğŸ“‹ Ground truth formatÄ±:")
                print("   KlasÃ¶r yapÄ±sÄ±: audio_dir/enstruman_adÄ±/dosyalar.wav")
                print("   Veya manuel etiket dosyasÄ±: labels.txt (dosya_adÄ±:etiket)")

                gt_method = input(
                    "\nYapÄ±: 1=KlasÃ¶r yapÄ±sÄ±, 2=Etiket dosyasÄ± (1/2): "
                ).strip()

                if gt_method == "1":
                    # KlasÃ¶r yapÄ±sÄ±ndan otomatik
                    ground_truth_map = {}
                    audio_dir_path = Path(audio_dir)
                    for audio_file in audio_dir_path.rglob("*.wav"):
                        # EÄŸer dosya bir alt klasÃ¶rdeyse, klasÃ¶r adÄ± = etiket
                        if audio_file.parent != audio_dir_path:
                            ground_truth_map[audio_file.name] = audio_file.parent.name
                    print(
                        f"âœ… {len(ground_truth_map)} dosya iÃ§in ground truth tespit edildi"
                    )

                elif gt_method == "2":
                    label_file = input("Etiket dosyasÄ± yolu: ").strip()
                    if Path(label_file).exists():
                        ground_truth_map = {}
                        with open(label_file, "r") as f:
                            for line in f:
                                if ":" in line:
                                    filename, label = line.strip().split(":", 1)
                                    ground_truth_map[filename] = label
                        print(f"âœ… {len(ground_truth_map)} etiket yÃ¼klendi")

            try:
                summary = classifier.batch_predict(
                    audio_dir=audio_dir,
                    model_path=selected_model["path"],
                    model_type=selected_model["type"],
                    ground_truth_map=ground_truth_map,
                )

                print("\n" + "=" * 70)
                print("âœ… TOPLU TAHMÄ°N TAMAMLANDI")
                print("=" * 70)
                print(f"ğŸ“Š Toplam dosya: {summary['total_files']}")
                print(f"âœ… BaÅŸarÄ±lÄ±: {summary['successful_predictions']}")
                print(f"âŒ BaÅŸarÄ±sÄ±z: {summary['failed_predictions']}")

                if "evaluation" in summary:
                    print(f"\nğŸ“ˆ Model PerformansÄ±:")
                    print(f"   Accuracy: {summary['evaluation']['accuracy']:.2%}")

            except Exception as e:
                print(f"âŒ Toplu tahmin hatasÄ±: {e}")

        else:
            print("âŒ GeÃ§ersiz seÃ§im!")

    # ==================== Ã‡IKIÅ ====================
    elif mode == "3":
        print("ğŸ‘‹ Ã‡Ä±kÄ±ÅŸ yapÄ±lÄ±yor...")
        return

    else:
        print("âŒ GeÃ§ersiz seÃ§im!")


if __name__ == "__main__":
    main()
